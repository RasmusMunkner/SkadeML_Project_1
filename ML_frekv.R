library(mlr3)
library(mvtnorm)
library(mlr3learners )
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(xgboost)
library(ranger)
library(mgcv)
library(mlr3extralearners)
library(tidyverse)
source("Rasmus_Funktioner.R") #For the ReadData-function
#Here data is freq_df, generated by "ML_datasets_Albert.R"

#Model 1, exposure as a feature.

#We want to use binary classification on the following subset.

#Read in the data for the frequency model, filter out ObsFreq and convert to a MLR3 task.
task_mod1 <- ReadData("freq_df") %>% 
  dplyr::select(-ObsFreq) %>%
  as_task_classif(target = "ClaimInd")

my_ranger_learner = lrn("classif.ranger",
                        mtry = to_tune(2, 8), #Seems to matter, but not super clearly
                        min.node.size = 1,
                        num.trees = to_tune(15, 150), #Seems to be completely irrelevant
                        max.depth = to_tune(2,8), #Something magical happens around 9 or 10, which may be where overfitting begins
                        predict_type= "prob")

my_ranger_learner_autotuner <- auto_tuner(
  method = tnr("random_search"),
  learner = my_ranger_learner,
  resampling = rsmp("cv", folds = 3),
  measure = msr("classif.ce"),
  terminator = trm("evals", n_evals = 50)
)

outer_resampling = rsmp("cv", folds = 8)

nestedcvResults <- resample(task_mod1, my_ranger_learner_autotuner, outer_resampling, store_models = T)

#Results from each fold from the outer resampling
nestedcvResults %>% 
  extract_inner_tuning_results() %>% 
  `[`(,list(iteration, mtry, max.depth, num.trees)) %>% 
  as_tibble() %>% 
  inner_join(nestedcvResults$score()[,list(iteration, classif.ce)] %>% as_tibble(), by = "iteration")

#CV estimate for generalization error
nestedcvResults$aggregate()

#Train the final model
my_ranger_learner_autotuner$train(task_mod1)

#Calculate the in-sample error - If this is much lower than the generalization error, something is probably off
my_ranger_learner_autotuner$predict(task_mod1)$score(msr("classif.ce"))

#Inspect the parameters fitted for the final model (will probably be deleted later)
my_ranger_learner_autotuner$learner$param_set

#Summarise results
results <- ReadData("freq_df") %>%
  add_column(Pred_freq_at = my_ranger_learner_autotuner$predict_newdata(.)$prob[,2])

results %>% 
  ggplot(aes(x = Exposure, y = Pred_freq_at)) +
  geom_point(aes(color = ClaimInd)) +
  geom_smooth(method = "gam") +
  geom_smooth(method = "lm", color = "red")

results %>% 
  group_by(ClaimInd) %>% 
  summarise(observedFreq = mean(ClaimInd %>% as.character() %>% as.numeric()), estFreq = mean(Pred_freq_at))






