library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(xgboost)
library(ranger)
library(mgcv)
library(mlr3extralearners)
library(fastDummies)


#Here data is claimsize_df, generated by "ML_datasets_Albert.R"

#Model 1: Elastic Net


########mlr: set task

task_elasticnet = as_task_regr(dummy_claim_df, target = "ClaimAmount") 


############  set learner with search space

my_elasticnet_learner = lrn("regr.glmnet",
                            s= to_tune(0, 1),
                            alpha=to_tune(0, 1))


#############  hyperparameter tuning setting
{start <- Sys.time()
  n<-200
  instance_elasticnet = tune(
    method = tnr("random_search"), ### tuning method
    task = task_elasticnet,
    learner = my_elasticnet_learner,
    resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
    measures = msr("regr.rmse"), #### root mean squared error
    terminator = trm("evals", n_evals=n) #### terminator
)
end<-Sys.time()
print( end - start )
}
#n=200, 19.13 secs

#### define tuned elasticnet learner and train on training data

elasticnet_tuned = lrn("regr.glmnet")  
elasticnet_tuned$param_set$values = instance_elasticnet$result_learner_param_vals
elasticnet_tuned$train(task_elasticnet)

dummy_df <- dummy_df %>% select(-c(Exposure,ClaimInd,ObsFreq)) %>%
  mutate(ClaimAmount = dummy_ClaimAmount)

final_df<- final_df %>%
  mutate("pred. claim elastic net" = elasticnet_tuned$predict_newdata(dummy_df)$response)

#Her bør man overveje om ikke vi skal prædiktere på det fulde datasæt, i.e.
#På freq_df, dvs. skrive freq_df i predict_newdata(.)


#Model 2 gradient boost

#Først skal der her foretages noget dummyvariable encoding
#af datasættet for at man kan lave regression med elasticnet.

task_xgboost = as_task_regr(dummy_claim_df, target = "ClaimAmount") 

my_xgb_learner = lrn("regr.xgboost",
                     eta = to_tune(0, 0.2),
                     nrounds = to_tune(10, 5000),
                     max_depth = to_tune(1, 3))


#############  hyperparameter tuning setting
{start <- Sys.time()
  n<-10
instance_xgb = tune(
  method = tnr("random_search"),
  task = task_xgboost,
  learner = my_xgb_learner,
  resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
  measures = msr("regr.rmse"), #### root mean squared error
  terminator = trm("evals", n_evals = n) #### terminator
)
end<-Sys.time()
print( end - start )
}
#n=10, 1,5 min
#### define tuned xgb learner and train on training data

xgb_tuned = lrn("regr.xgboost")  
xgb_tuned$param_set$values = instance_xgb$result_learner_param_vals
xgb_tuned$train(task_xgboost)

final_df<- final_df %>%
  mutate("pred. claim xgboost" = xgb_tuned$predict_newdata(dummy_df)$response)


#Model 3 random forest

task_ranger_skd = as_task_regr(dummy_claim_df, target = "ClaimAmount") 

my_ranger_learner_skd = lrn("regr.ranger",
                        mtry.ratio = to_tune(0.1,1),
                        min.node.size = to_tune(1, 50),
                        num.trees = 50)

#############  hyperparameter tuning setting
{start <- Sys.time()
n<-10
instance_ranger_skd = tune(
  method = tnr("random_search"),
  task = task_ranger_skd,
  learner = my_ranger_learner_skd,
  resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
  measures = msr("regr.rmse"), #### root mean squared error
  terminator = trm("evals", n_evals = n) #### terminator
)
end<-Sys.time()
print( end - start )
}
#n=10, 7,3s
#### define tuned ranger learner and train on training data

ranger_tuned_skd = lrn("regr.ranger")  
ranger_tuned_skd$param_set$values = instance_ranger_skd$result_learner_param_vals
ranger_tuned_skd$train(task_ranger_skd)

final_df<- final_df %>%
  mutate("pred. claim random forest" = ranger_tuned_skd$predict_newdata(dummy_df)$response)





