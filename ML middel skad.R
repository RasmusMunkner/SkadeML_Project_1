library(mlr3)
library(mlr3learners)
library(mlr3tuning)
library(mlr3mbo)
library(glmnet)
library(xgboost)
library(ranger)
library(mgcv)
library(mlr3extralearners)


#Here data is claimsize_df, generated by "ML_datasets_Albert.R"

#Model 1: Elastic Net

#Først skal der her foretages noget dummyvariable encoding
#af datasættet for at man kan lave regression med elasticnet.

########mlr: set task

task_elasticnet = as_task_regr(claimsize_df, target = "ClaimAmount") 


############  set learner with search space

my_elasticnet_learner = lrn("regr.glmnet",
                            s= to_tune(0, 1),
                            alpha=to_tune(0, 1))


#############  hyperparameter tuning setting
instance_elasticnet = tune(
  method = tnr("random_search"), ### tuning method
  task = task_elasticnet,
  learner = my_elasticnet_learner,
  resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
  measures = msr("regr.rmse"), #### root mean squared error
  terminator = trm("evals", n_evals = 50) #### terminator
)

#### define tuned elasticnet learner and train on training data

elasticnet_tuned = lrn("regr.glmnet")  
elasticnet_tuned$param_set$values = instance_elasticnet$result_learner_param_vals
elasticnet_tuned$train(task_elasticnet)

claimsize_df<- claimsize_df %>%
  mutate("pred. claim elastic net" = elasticnet_tuned$predict_newdata(claimsize_df)$response)

#Her bør man overveje om ikke vi skal prædiktere på det fulde datasæt, i.e.
#På freq_df, dvs. skrive freq_df i predict_newdata(.)


#Model 2 gradient boost

#Først skal der her foretages noget dummyvariable encoding
#af datasættet for at man kan lave regression med elasticnet.

task_xgboost = as_task_regr(claimsize_df, target = "ClaimAmount") 

my_xgb_learner = lrn("regr.xgboost",
                     eta = to_tune(0, 0.2),
                     nrounds = to_tune(10, 5000),
                     max_depth = to_tune(1, 3))


#############  hyperparameter tuning setting
instance_xgb = tune(
  method = tnr("random_search"),
  task = task_xgboost,
  learner = my_xgb_learner,
  resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
  measures = msr("regr.rmse"), #### root mean squared error
  terminator = trm("evals", n_evals = 50) #### terminator
)

#### define tuned xgb learner and train on training data

xgb_tuned = lrn("regr.xgboost")  
xgb_tuned$param_set$values = instance_xgb$result_learner_param_vals
xgb_tuned$train(task_xgboost)

claimsize_df<- claimsize_df %>%
  mutate("pred. claim xgboost" = xgb_tuned$predict_newdata(claimsize_df)$response)

#Her bør man overveje om ikke vi skal prædiktere på det fulde datasæt, i.e.
#På freq_df, dvs. skrive freq_df i predict_newdata(.)


#Model 3 random forest

#Først skal der her foretages noget dummyvariable encoding
#af datasættet for at man kan lave regression med elasticnet.

task_ranger_skd = as_task_regr(claimsize_df, target = "ClaimAmount") 

my_ranger_learner_skd = lrn("regr.ranger",
                        mtry.ratio = to_tune(0.1,1),
                        min.node.size = to_tune(1, 50),
                        num.trees = 50)

#############  hyperparameter tuning setting
instance_ranger_skd = tune(
  method = tnr("mbo"),
  task = task_ranger_skd,
  learner = my_ranger_learner_skd,
  resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
  measures = msr("regr.rmse"), #### root mean squared error
  terminator = trm("evals", n_evals = 50) #### terminator
)

#### define tuned ranger learner and train on training data

ranger_tuned_skd = lrn("regr.ranger")  
ranger_tuned_skd$param_set$values = instance_ranger_skd$result_learner_param_vals
ranger_tuned_skd$train(task_ranger_skd)

claimsize_df<- claimsize_df %>%
  mutate("pred. claim random forest" = ranger_tuned_skd$predict_newdata(claimsize_df)$response)

#Her bør man overveje om ikke vi skal prædiktere på det fulde datasæt, i.e.
#På freq_df, dvs. skrive freq_df i predict_newdata(.)



